{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer, one_hot\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "import keras.optimizers\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn.datasets as skds\n",
    "from pathlib import Path\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "\n",
    "max_letters=12\n",
    "char_count=104"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source file directory\n",
    "path_train = \"./training\"\n",
    " \n",
    "files_train = skds.load_files(path_train,load_content=False)\n",
    " \n",
    "label_index = files_train.target\n",
    "label_names = files_train.target_names\n",
    "labelled_files = files_train.filenames\n",
    "\n",
    "# data_tags = [\"filename\",\"language\",\"wikitext\"]\n",
    "data_tags = [\"language\",\"wikitext\"]\n",
    "data_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['language', 'wikitext']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and add data from file to a list\n",
    "i=0\n",
    "for f in labelled_files:\n",
    "    # data_list.append((f,label_names[label_index[i]],Path(f).read_text(encoding=\"utf8\")))\n",
    "    data_list.append((label_names[label_index[i]],Path(f).read_text(encoding=\"utf8\")))\n",
    "    i += 1\n",
    " \n",
    "# We have training data available as dictionary filename, category, data\n",
    "data = pd.DataFrame.from_records(data_list, columns=data_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>wikitext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>english</td>\n",
       "      <td>Zealand and Australia for the first time The b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>french</td>\n",
       "      <td>parlement de Bosnie-Herzégovine Les électeurs ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>german</td>\n",
       "      <td>die westliche Linke könne die Sowjetunion kein...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>romanian</td>\n",
       "      <td>în secolul al XIV-lea și comiți ai comitatului...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   language                                           wikitext\n",
       "0   english  Zealand and Australia for the first time The b...\n",
       "1    french  parlement de Bosnie-Herzégovine Les électeurs ...\n",
       "2    german  die westliche Linke könne die Sowjetunion kein...\n",
       "3  romanian  în secolul al XIV-lea și comiți ai comitatului..."
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_english = data.loc[data['language'] == \"english\"].reset_index(drop=True)\n",
    "data_german = data.loc[data['language'] == \"german\"].reset_index(drop=True)\n",
    "data_french = data.loc[data['language'] == \"french\"].reset_index(drop=True)\n",
    "data_romanian = data.loc[data['language'] == \"romanian\"].reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "data_sorted=pd.concat([data_english, data_german, data_french, data_romanian]).sort_index(kind='mergesort').reset_index(drop=True)\n",
    "data_sorted.head(20)\n",
    "data_english.head()\n",
    "data_english_m=data_english.groupby('language')['wikitext'].apply(' '.join).reset_index()\n",
    "data_french_m=data_french.groupby('language')['wikitext'].apply(' '.join).reset_index()\n",
    "data_german_m=data_german.groupby('language')['wikitext'].apply(' '.join).reset_index()\n",
    "data_romanian_m=data_romanian.groupby('language')['wikitext'].apply(' '.join).reset_index()\n",
    "\n",
    "data_grouped=data.groupby('language')['wikitext'].apply(' '.join).reset_index()\n",
    "data_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n"
     ]
    }
   ],
   "source": [
    "def process(page_content, max_word_length):\n",
    "    count=0\n",
    "    #page_content = unidecode(page_content)\n",
    "    words = re.sub(r'[^a-zA-Z ]', ' ', page_content)\n",
    "    lower = words.lower()\n",
    "    word_list = lower.split()\n",
    "    short_words = []\n",
    "    for word in word_list:\n",
    "        if count >= 20000:\n",
    "            break\n",
    "        if len(word) <= max_word_length and len(word) > 2:\n",
    "            short_words.append(word)\n",
    "            count=count+1\n",
    "    \n",
    "    #short_words=list(set(short_words))\n",
    "    short_words.sort()\n",
    "    return short_words\n",
    "\n",
    "test= process(data_grouped.loc[data_grouped.language == 'english','wikitext'].tolist()[0], 12)\n",
    "print(len(test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dic_to_vector(dic, max_word_length):\n",
    "    new_list = []\n",
    "    for word in dic:\n",
    "        vec = ''\n",
    "        n = len(word)\n",
    "        for i in range(n):\n",
    "            current_letter = word[i]\n",
    "            ind = ord(current_letter)-97\n",
    "            #ind = ord(current_letter)\n",
    "            placeholder = (str(0)*ind) + str(1) + str(0)*((char_count-1)-ind)\n",
    "            vec = vec + placeholder\n",
    "        if n < max_word_length:\n",
    "            excess = max_word_length-n\n",
    "            vec = vec +str(0)*char_count*excess\n",
    "        new_list.append(vec)\n",
    "    print(len(new_list))\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord(\"ă\")-97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_output_vector(tag_index, number_of_languages):\n",
    "    out = str(0)*tag_index + str(1) + str(0)*(number_of_languages-1-tag_index)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating dictionary for english\n",
      "20000\n",
      "generating dictionary for french\n",
      "20000\n",
      "generating dictionary for german\n",
      "20000\n",
      "generating dictionary for romanian\n",
      "20000\n"
     ]
    }
   ],
   "source": [
    "word_data = []\n",
    "language_data = []\n",
    "master_dic = []\n",
    "\n",
    "count = 0\n",
    "\n",
    "\n",
    "for lang in label_names:\n",
    "    print('generating dictionary for ' + lang)\n",
    "    dic = process(data_grouped.loc[data_grouped.language == lang,'wikitext'].tolist()[0], max_letters)\n",
    "    for word in dic:\n",
    "        master_dic.append(word)\n",
    "    vct = convert_dic_to_vector(dic, max_letters)\n",
    "    for vector in vct:\n",
    "        word_data.append(vector)\n",
    "    output_vct = create_output_vector(count, len(label_names))\n",
    "    for i in range(len(vct)):\n",
    "        language_data.append(output_vct)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_grouped.loc[data_grouped.language == \"romanian\",'wikitext'].tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = []\n",
    "for i in range(len(word_data)):\n",
    "    entry = []\n",
    "    entry.append(master_dic[i])\n",
    "    for digit in language_data[i]:\n",
    "        entry.append(float(digit))\n",
    "    for digit in word_data[i]:\n",
    "        entry.append(float(digit))\n",
    "    arr.append(entry)\n",
    "    #print(entry)\n",
    "\n",
    "\n",
    "arr = np.array(arr)\n",
    "np.save('arr.npy', arr)\n",
    "df=pd.DataFrame(arr)\n",
    "df.to_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1243</th>\n",
       "      <th>1244</th>\n",
       "      <th>1245</th>\n",
       "      <th>1246</th>\n",
       "      <th>1247</th>\n",
       "      <th>1248</th>\n",
       "      <th>1249</th>\n",
       "      <th>1250</th>\n",
       "      <th>1251</th>\n",
       "      <th>1252</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abandoned</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abandoned</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abandoned</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abbie</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abbreviated</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1253 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0    1    2    3    4    5    6    7    8    9     ... 1243 1244  \\\n",
       "0    abandoned  1.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "1    abandoned  1.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "2    abandoned  1.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "3        abbie  1.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "4  abbreviated  1.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "\n",
       "  1245 1246 1247 1248 1249 1250 1251 1252  \n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 1253 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('arr.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 1247)\n",
      "(12000, 4)\n",
      "(68000, 1247)\n",
      "(68000, 4)\n"
     ]
    }
   ],
   "source": [
    "#data = np.load('arr.npy')\n",
    "\n",
    "inputs = data[:, 2+len(label_names):]\n",
    "labels = data[:, 1:1+len(label_names)]\n",
    "\n",
    "#inputs = data[:, 6:]\n",
    "#labels = data[:, 1:5]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(inputs, labels, test_size=0.15)\n",
    "\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Sequential()\n",
    "network.add(Dense(200, input_dim=(char_count*max_letters)-1, activation='sigmoid'))\n",
    "network.add(Dense(150, activation='sigmoid'))\n",
    "network.add(Dense(100, activation='sigmoid'))\n",
    "network.add(Dense(100, activation='sigmoid'))\n",
    "network.add(Dense(len(label_names), activation='softmax'))\n",
    "\n",
    "network.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 68000 samples, validate on 12000 samples\n",
      "Epoch 1/1\n",
      "68000/68000 [==============================] - 56s 825us/step - loss: 0.5633 - accuracy: 0.7500 - val_loss: 0.5614 - val_accuracy: 0.7500\n",
      "\n",
      "Epoch 00001: saving model to weights.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2af395b3080>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = \"weights.hdf5\"\n",
    "#checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, mode='max')\n",
    "tboard = TensorBoard(log_dir='logs', write_graph=True, write_images=True)\n",
    "callbacks_list = [checkpoint, tboard]\n",
    "\n",
    "network.fit(x_train, y_train, epochs=1, batch_size=1000, validation_data=(x_test, y_test), callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1144\n",
      "english: 26.23%\n",
      "french: 24.85%\n",
      "german: 22.58%\n",
      "romanian: 26.33%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    #ă ü\n",
    "    dic = []\n",
    "    dic.append(\"hello\")\n",
    "    vct_str = convert_dic_to_vector(dic, max_letters-1)\n",
    "    vct = np.zeros((1, (char_count * max_letters)-1))\n",
    "    count = 0\n",
    "    print(len(vct_str[0]))\n",
    "    for digit in vct_str[0]:\n",
    "        vct[0,count] = int(digit)\n",
    "        count += 1\n",
    "    prediction_vct = network.predict(vct)\n",
    "\n",
    "    langs = list(label_names)\n",
    "    for i in range(len(label_names)):\n",
    "        lang = langs[i]\n",
    "        score = prediction_vct[0][i]\n",
    "        print(lang + ': ' + str(round(100*score, 2)) + '%')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.save('lang_detect.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "del network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = load_model('lang_detect.hdf5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-pythondata] *",
   "language": "python",
   "name": "conda-env-.conda-pythondata-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
