{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer, one_hot\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "import keras.optimizers\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn.datasets as skds\n",
    "from pathlib import Path\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import os\n",
    "from unidecode import unidecode\n",
    "from keras import layers\n",
    "\n",
    "max_letters=12\n",
    "char_count=26\n",
    "#char_count=104"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['English', 'French', 'German', 'Romanian']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>wikitext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>English</td>\n",
       "      <td>History of the Jews in Romania The history of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>French</td>\n",
       "      <td>La Roche-sur-Yon La Roche-sur-Yon est une com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>German</td>\n",
       "      <td>Paris Paris ist die Hauptstadt der Französisc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Romanian</td>\n",
       "      <td>Avrig Avrig în dialectul săsesc Frek Fraek în...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   language                                           wikitext\n",
       "0   English   History of the Jews in Romania The history of...\n",
       "1    French   La Roche-sur-Yon La Roche-sur-Yon est une com...\n",
       "2    German   Paris Paris ist die Hauptstadt der Französisc...\n",
       "3  Romanian   Avrig Avrig în dialectul săsesc Frek Fraek în..."
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_names = os.listdir('./training/clean/')\n",
    "# Create Dictionary for language (= File name) and Text -- rename the files such as: en_clean.txt = english etc.\n",
    "file_name_and_text = {}\n",
    "for file in file_names:\n",
    "    with open('./training/clean/' + file, \"r\",encoding=\"utf\" ) as target_file:\n",
    "         file_name_and_text[file] = target_file.read()\n",
    "data_grouped = (pd.DataFrame.from_dict(file_name_and_text, orient='index')\n",
    "             .reset_index().rename(index = str, columns = {'index': 'language', 0: 'wikitext'}))\n",
    "label_names = data_grouped['language'].tolist()\n",
    "print(label_names)\n",
    "data_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(page_content, max_word_length):\n",
    "    count=0\n",
    "    # unidecode will replace special chars like ă or ü with a or u\n",
    "    #page_content = unidecode(page_content)\n",
    "    words = re.sub(r'[^a-zA-Z ]', ' ', page_content)\n",
    "    lower = words.lower()\n",
    "    word_list = lower.split()\n",
    "    short_words = []\n",
    "    for word in word_list:\n",
    "        # set to lower number for testing, limits total number of words used\n",
    "        if count >= 100000:\n",
    "            break\n",
    "            # and len(word) > 2\n",
    "        if len(word) <= max_word_length and len(word) > 1:\n",
    "            short_words.append(word)\n",
    "            count=count+1\n",
    "    \n",
    "    # sorting may be better for machine learning but will cause loss of end of alphabet words if full list is not used\n",
    "    #short_words=list(set(short_words))\n",
    "    #short_words.sort()\n",
    "    return short_words\n",
    "\n",
    "#test= process(data_grouped.loc[data_grouped.language == 'English','wikitext'].tolist()[0], 12)\n",
    "#print(len(test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode words here\n",
    "def convert_dic_to_vector(dic, max_word_length):\n",
    "    new_list = []\n",
    "    for word in dic:\n",
    "        vec = ''\n",
    "        n = len(word)\n",
    "        for i in range(n):\n",
    "            current_letter = word[i]\n",
    "            ind = ord(current_letter)-97\n",
    "            #ind = ord(current_letter)\n",
    "            placeholder = (str(0)*ind) + str(1) + str(0)*((char_count-1)-ind)\n",
    "            vec = vec + placeholder\n",
    "        if n < max_word_length:\n",
    "            excess = max_word_length-n\n",
    "            vec = vec +str(0)*char_count*excess\n",
    "        new_list.append(vec)\n",
    "    print(len(new_list))\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test special charactor values here\n",
    "ord(\"ă\")-97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode words here\n",
    "def create_output_vector(tag_index, number_of_languages):\n",
    "    out = str(0)*tag_index + str(1) + str(0)*(number_of_languages-1-tag_index)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating dictionary for English\n",
      "100000\n",
      "generating dictionary for French\n",
      "100000\n",
      "generating dictionary for German\n",
      "100000\n",
      "generating dictionary for Romanian\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "# build dictionaries to train from\n",
    "word_data = []\n",
    "language_data = []\n",
    "master_dic = []\n",
    "\n",
    "count = 0\n",
    "\n",
    "for lang in label_names:\n",
    "    print('generating dictionary for ' + lang)\n",
    "    dic = process(data_grouped.loc[data_grouped.language == lang,'wikitext'].tolist()[0], max_letters)\n",
    "    for word in dic:\n",
    "        master_dic.append(word)\n",
    "    vct = convert_dic_to_vector(dic, max_letters)\n",
    "    for vector in vct:\n",
    "        word_data.append(vector)\n",
    "    output_vct = create_output_vector(count, len(label_names))\n",
    "    for i in range(len(vct)):\n",
    "        language_data.append(output_vct)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_grouped.loc[data_grouped.language == \"romanian\",'wikitext'].tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoded dataframe\n",
    "arr = []\n",
    "for i in range(len(word_data)):\n",
    "    entry = []\n",
    "    entry.append(master_dic[i])\n",
    "    for digit in language_data[i]:\n",
    "        entry.append(float(digit))\n",
    "    for digit in word_data[i]:\n",
    "        entry.append(float(digit))\n",
    "    arr.append(entry)\n",
    "    #print(entry)\n",
    "\n",
    "#uncomment to save large arr\n",
    "#arr = np.array(arr)\n",
    "#np.save('arr.npy', arr)\n",
    "#df=pd.DataFrame(arr)\n",
    "#df.to_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df=pd.DataFrame(arr)\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = np.load('arr.npy')\n",
    "data = np.array(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80000, 311)\n",
      "(80000, 4)\n",
      "(320000, 311)\n",
      "(320000, 4)\n"
     ]
    }
   ],
   "source": [
    "#split into train and test, verify array shapes\n",
    "\n",
    "inputs = data[:, 2+len(label_names):]\n",
    "labels = data[:, 1:1+len(label_names)]\n",
    "\n",
    "#inputs = data[:, 6:]\n",
    "#labels = data[:, 1:5]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(inputs, labels, test_size=0.20)\n",
    "\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the brain\n",
    "network = Sequential()\n",
    "network.add(Dense(512, input_dim=(char_count*max_letters)-1))\n",
    "network.add(Activation('relu'))\n",
    "network.add(Dropout(0.5))\n",
    "network.add(Dense(512, activation='sigmoid'))\n",
    "network.add(Dropout(0.4))\n",
    "network.add(Dense(512, activation='sigmoid'))\n",
    "network.add(Dropout(0.3))\n",
    "network.add(Dense(len(label_names), activation='softmax'))\n",
    "\n",
    "network.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network = Sequential()\n",
    "# network.add(Dense(512, input_shape=((char_count*max_letters)-1,)))\n",
    "# network.add(Activation('relu'))\n",
    "# network.add(Dropout(0.3))\n",
    "# network.add(Dense(512))\n",
    "# network.add(Activation('relu'))\n",
    "# network.add(Dropout(0.3))\n",
    "# network.add(Dense(len(label_names)))\n",
    "# network.add(Activation('softmax'))\n",
    "# network.summary()\n",
    " \n",
    "# network.compile(loss='categorical_crossentropy',\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])\n",
    " \n",
    "# history = network.fit(x_train, y_train,\n",
    "#                     batch_size=1000,\n",
    "#                     epochs=30,\n",
    "#                     verbose=1,\n",
    "#                     validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 320000 samples, validate on 80000 samples\n",
      "Epoch 1/100\n",
      "320000/320000 [==============================] - 92s 289us/step - loss: 0.4067 - accuracy: 0.8204 - val_loss: 0.2773 - val_accuracy: 0.8856\n",
      "\n",
      "Epoch 00001: saving model to weights.hdf5\n",
      "Epoch 2/100\n",
      "320000/320000 [==============================] - 89s 279us/step - loss: 0.2796 - accuracy: 0.8827 - val_loss: 0.2290 - val_accuracy: 0.9059\n",
      "\n",
      "Epoch 00002: saving model to weights.hdf5\n",
      "Epoch 3/100\n",
      "320000/320000 [==============================] - 91s 284us/step - loss: 0.2487 - accuracy: 0.8963 - val_loss: 0.2096 - val_accuracy: 0.9143\n",
      "\n",
      "Epoch 00003: saving model to weights.hdf5\n",
      "Epoch 4/100\n",
      "320000/320000 [==============================] - 87s 273us/step - loss: 0.2325 - accuracy: 0.9033 - val_loss: 0.1988 - val_accuracy: 0.9187\n",
      "\n",
      "Epoch 00004: saving model to weights.hdf5\n",
      "Epoch 5/100\n",
      "320000/320000 [==============================] - 88s 274us/step - loss: 0.2214 - accuracy: 0.9078 - val_loss: 0.1902 - val_accuracy: 0.9219\n",
      "\n",
      "Epoch 00005: saving model to weights.hdf5\n",
      "Epoch 6/100\n",
      "320000/320000 [==============================] - 89s 278us/step - loss: 0.2149 - accuracy: 0.9107 - val_loss: 0.1866 - val_accuracy: 0.9239\n",
      "\n",
      "Epoch 00006: saving model to weights.hdf5\n",
      "Epoch 7/100\n",
      "320000/320000 [==============================] - 88s 276us/step - loss: 0.2079 - accuracy: 0.9138 - val_loss: 0.1812 - val_accuracy: 0.9252\n",
      "\n",
      "Epoch 00007: saving model to weights.hdf5\n",
      "Epoch 8/100\n",
      "320000/320000 [==============================] - 88s 274us/step - loss: 0.2030 - accuracy: 0.9158 - val_loss: 0.1781 - val_accuracy: 0.9268\n",
      "\n",
      "Epoch 00008: saving model to weights.hdf5\n",
      "Epoch 9/100\n",
      "320000/320000 [==============================] - 89s 278us/step - loss: 0.1980 - accuracy: 0.9182 - val_loss: 0.1757 - val_accuracy: 0.9284\n",
      "\n",
      "Epoch 00009: saving model to weights.hdf5\n",
      "Epoch 10/100\n",
      "320000/320000 [==============================] - 89s 279us/step - loss: 0.1949 - accuracy: 0.9188 - val_loss: 0.1739 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00010: saving model to weights.hdf5\n",
      "Epoch 11/100\n",
      "320000/320000 [==============================] - 86s 269us/step - loss: 0.1922 - accuracy: 0.9201 - val_loss: 0.1708 - val_accuracy: 0.9300\n",
      "\n",
      "Epoch 00011: saving model to weights.hdf5\n",
      "Epoch 12/100\n",
      "320000/320000 [==============================] - 85s 265us/step - loss: 0.1889 - accuracy: 0.9216 - val_loss: 0.1694 - val_accuracy: 0.9309\n",
      "\n",
      "Epoch 00012: saving model to weights.hdf5\n",
      "Epoch 13/100\n",
      "320000/320000 [==============================] - 86s 268us/step - loss: 0.1860 - accuracy: 0.9229 - val_loss: 0.1683 - val_accuracy: 0.9313\n",
      "\n",
      "Epoch 00013: saving model to weights.hdf5\n",
      "Epoch 14/100\n",
      "320000/320000 [==============================] - 88s 274us/step - loss: 0.1840 - accuracy: 0.9236 - val_loss: 0.1657 - val_accuracy: 0.9315\n",
      "\n",
      "Epoch 00014: saving model to weights.hdf5\n",
      "Epoch 15/100\n",
      "320000/320000 [==============================] - 90s 282us/step - loss: 0.1822 - accuracy: 0.9244 - val_loss: 0.1652 - val_accuracy: 0.9323\n",
      "\n",
      "Epoch 00015: saving model to weights.hdf5\n",
      "Epoch 16/100\n",
      "320000/320000 [==============================] - 91s 284us/step - loss: 0.1804 - accuracy: 0.9248 - val_loss: 0.1641 - val_accuracy: 0.9326\n",
      "\n",
      "Epoch 00016: saving model to weights.hdf5\n",
      "Epoch 17/100\n",
      "320000/320000 [==============================] - 90s 280us/step - loss: 0.1784 - accuracy: 0.9257 - val_loss: 0.1629 - val_accuracy: 0.9329\n",
      "\n",
      "Epoch 00017: saving model to weights.hdf5\n",
      "Epoch 18/100\n",
      "320000/320000 [==============================] - 89s 278us/step - loss: 0.1763 - accuracy: 0.9267 - val_loss: 0.1617 - val_accuracy: 0.9330\n",
      "\n",
      "Epoch 00018: saving model to weights.hdf5\n",
      "Epoch 19/100\n",
      "320000/320000 [==============================] - 89s 279us/step - loss: 0.1750 - accuracy: 0.9272 - val_loss: 0.1613 - val_accuracy: 0.9336\n",
      "\n",
      "Epoch 00019: saving model to weights.hdf5\n",
      "Epoch 20/100\n",
      "320000/320000 [==============================] - 90s 280us/step - loss: 0.1735 - accuracy: 0.9277 - val_loss: 0.1599 - val_accuracy: 0.9337\n",
      "\n",
      "Epoch 00020: saving model to weights.hdf5\n",
      "Epoch 21/100\n",
      "320000/320000 [==============================] - 94s 295us/step - loss: 0.1723 - accuracy: 0.9282 - val_loss: 0.1595 - val_accuracy: 0.9339\n",
      "\n",
      "Epoch 00021: saving model to weights.hdf5\n",
      "Epoch 22/100\n",
      "320000/320000 [==============================] - 88s 276us/step - loss: 0.1707 - accuracy: 0.9289 - val_loss: 0.1583 - val_accuracy: 0.9339\n",
      "\n",
      "Epoch 00022: saving model to weights.hdf5\n",
      "Epoch 23/100\n",
      "320000/320000 [==============================] - 89s 278us/step - loss: 0.1705 - accuracy: 0.9289 - val_loss: 0.1582 - val_accuracy: 0.9342\n",
      "\n",
      "Epoch 00023: saving model to weights.hdf5\n",
      "Epoch 24/100\n",
      "320000/320000 [==============================] - 90s 282us/step - loss: 0.1684 - accuracy: 0.9298 - val_loss: 0.1570 - val_accuracy: 0.9347\n",
      "\n",
      "Epoch 00024: saving model to weights.hdf5\n",
      "Epoch 25/100\n",
      "320000/320000 [==============================] - 90s 281us/step - loss: 0.1675 - accuracy: 0.9300 - val_loss: 0.1565 - val_accuracy: 0.9350\n",
      "\n",
      "Epoch 00025: saving model to weights.hdf5\n",
      "Epoch 26/100\n",
      "320000/320000 [==============================] - 90s 282us/step - loss: 0.1668 - accuracy: 0.9304 - val_loss: 0.1562 - val_accuracy: 0.9349\n",
      "\n",
      "Epoch 00026: saving model to weights.hdf5\n",
      "Epoch 27/100\n",
      "320000/320000 [==============================] - 90s 281us/step - loss: 0.1657 - accuracy: 0.9309 - val_loss: 0.1559 - val_accuracy: 0.9354\n",
      "\n",
      "Epoch 00027: saving model to weights.hdf5\n",
      "Epoch 28/100\n",
      "320000/320000 [==============================] - 89s 279us/step - loss: 0.1649 - accuracy: 0.9311 - val_loss: 0.1554 - val_accuracy: 0.9353\n",
      "\n",
      "Epoch 00028: saving model to weights.hdf5\n",
      "Epoch 29/100\n",
      "320000/320000 [==============================] - 87s 271us/step - loss: 0.1633 - accuracy: 0.9320 - val_loss: 0.1550 - val_accuracy: 0.9354\n",
      "\n",
      "Epoch 00029: saving model to weights.hdf5\n",
      "Epoch 30/100\n",
      "320000/320000 [==============================] - 88s 276us/step - loss: 0.1629 - accuracy: 0.9320 - val_loss: 0.1544 - val_accuracy: 0.9358\n",
      "\n",
      "Epoch 00030: saving model to weights.hdf5\n",
      "Epoch 31/100\n",
      "320000/320000 [==============================] - 87s 272us/step - loss: 0.1619 - accuracy: 0.9323 - val_loss: 0.1538 - val_accuracy: 0.9359\n",
      "\n",
      "Epoch 00031: saving model to weights.hdf5\n",
      "Epoch 32/100\n",
      "320000/320000 [==============================] - 87s 272us/step - loss: 0.1614 - accuracy: 0.9328 - val_loss: 0.1532 - val_accuracy: 0.9361\n",
      "\n",
      "Epoch 00032: saving model to weights.hdf5\n",
      "Epoch 33/100\n",
      "320000/320000 [==============================] - 89s 277us/step - loss: 0.1605 - accuracy: 0.9329 - val_loss: 0.1534 - val_accuracy: 0.9361\n",
      "\n",
      "Epoch 00033: saving model to weights.hdf5\n",
      "Epoch 34/100\n",
      "320000/320000 [==============================] - 91s 283us/step - loss: 0.1600 - accuracy: 0.9333 - val_loss: 0.1531 - val_accuracy: 0.9363\n",
      "\n",
      "Epoch 00034: saving model to weights.hdf5\n",
      "Epoch 35/100\n",
      "320000/320000 [==============================] - 89s 280us/step - loss: 0.1589 - accuracy: 0.9337 - val_loss: 0.1524 - val_accuracy: 0.9366\n",
      "\n",
      "Epoch 00035: saving model to weights.hdf5\n",
      "Epoch 36/100\n",
      "320000/320000 [==============================] - 91s 283us/step - loss: 0.1584 - accuracy: 0.9337 - val_loss: 0.1525 - val_accuracy: 0.9364\n",
      "\n",
      "Epoch 00036: saving model to weights.hdf5\n",
      "Epoch 37/100\n",
      "320000/320000 [==============================] - 91s 286us/step - loss: 0.1578 - accuracy: 0.9340 - val_loss: 0.1517 - val_accuracy: 0.9366\n",
      "\n",
      "Epoch 00037: saving model to weights.hdf5\n",
      "Epoch 38/100\n",
      "320000/320000 [==============================] - 90s 283us/step - loss: 0.1575 - accuracy: 0.9342 - val_loss: 0.1515 - val_accuracy: 0.9367\n",
      "\n",
      "Epoch 00038: saving model to weights.hdf5\n",
      "Epoch 39/100\n",
      "320000/320000 [==============================] - 91s 284us/step - loss: 0.1565 - accuracy: 0.9347 - val_loss: 0.1516 - val_accuracy: 0.9368\n",
      "\n",
      "Epoch 00039: saving model to weights.hdf5\n",
      "Epoch 40/100\n",
      "320000/320000 [==============================] - 92s 288us/step - loss: 0.1562 - accuracy: 0.9346 - val_loss: 0.1515 - val_accuracy: 0.9361\n",
      "\n",
      "Epoch 00040: saving model to weights.hdf5\n",
      "Epoch 41/100\n",
      "320000/320000 [==============================] - 90s 280us/step - loss: 0.1557 - accuracy: 0.9351 - val_loss: 0.1512 - val_accuracy: 0.9372\n",
      "\n",
      "Epoch 00041: saving model to weights.hdf5\n",
      "Epoch 42/100\n",
      "320000/320000 [==============================] - 89s 279us/step - loss: 0.1552 - accuracy: 0.9349 - val_loss: 0.1508 - val_accuracy: 0.9372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00042: saving model to weights.hdf5\n",
      "Epoch 43/100\n",
      "320000/320000 [==============================] - 91s 285us/step - loss: 0.1544 - accuracy: 0.9354 - val_loss: 0.1502 - val_accuracy: 0.9371\n",
      "\n",
      "Epoch 00043: saving model to weights.hdf5\n",
      "Epoch 44/100\n",
      "320000/320000 [==============================] - 90s 281us/step - loss: 0.1539 - accuracy: 0.9356 - val_loss: 0.1505 - val_accuracy: 0.9376\n",
      "\n",
      "Epoch 00044: saving model to weights.hdf5\n",
      "Epoch 45/100\n",
      "320000/320000 [==============================] - 95s 297us/step - loss: 0.1540 - accuracy: 0.9355 - val_loss: 0.1503 - val_accuracy: 0.9373\n",
      "\n",
      "Epoch 00045: saving model to weights.hdf5\n",
      "Epoch 46/100\n",
      "320000/320000 [==============================] - 98s 306us/step - loss: 0.1533 - accuracy: 0.9357 - val_loss: 0.1503 - val_accuracy: 0.9373\n",
      "\n",
      "Epoch 00046: saving model to weights.hdf5\n",
      "Epoch 47/100\n",
      "320000/320000 [==============================] - 93s 291us/step - loss: 0.1524 - accuracy: 0.9362 - val_loss: 0.1501 - val_accuracy: 0.9377\n",
      "\n",
      "Epoch 00047: saving model to weights.hdf5\n",
      "Epoch 48/100\n",
      "320000/320000 [==============================] - 96s 299us/step - loss: 0.1520 - accuracy: 0.9363 - val_loss: 0.1496 - val_accuracy: 0.9377\n",
      "\n",
      "Epoch 00048: saving model to weights.hdf5\n",
      "Epoch 49/100\n",
      "320000/320000 [==============================] - 91s 286us/step - loss: 0.1514 - accuracy: 0.9365 - val_loss: 0.1496 - val_accuracy: 0.9378\n",
      "\n",
      "Epoch 00049: saving model to weights.hdf5\n",
      "Epoch 50/100\n",
      "320000/320000 [==============================] - 90s 281us/step - loss: 0.1518 - accuracy: 0.9364 - val_loss: 0.1490 - val_accuracy: 0.9380\n",
      "\n",
      "Epoch 00050: saving model to weights.hdf5\n",
      "Epoch 51/100\n",
      "320000/320000 [==============================] - 94s 292us/step - loss: 0.1510 - accuracy: 0.9366 - val_loss: 0.1487 - val_accuracy: 0.9378\n",
      "\n",
      "Epoch 00051: saving model to weights.hdf5\n",
      "Epoch 52/100\n",
      "320000/320000 [==============================] - 90s 282us/step - loss: 0.1505 - accuracy: 0.9368 - val_loss: 0.1490 - val_accuracy: 0.9377\n",
      "\n",
      "Epoch 00052: saving model to weights.hdf5\n",
      "Epoch 53/100\n",
      "320000/320000 [==============================] - 88s 275us/step - loss: 0.1496 - accuracy: 0.9372 - val_loss: 0.1483 - val_accuracy: 0.9385\n",
      "\n",
      "Epoch 00053: saving model to weights.hdf5\n",
      "Epoch 54/100\n",
      "320000/320000 [==============================] - 90s 280us/step - loss: 0.1495 - accuracy: 0.9371 - val_loss: 0.1482 - val_accuracy: 0.9384\n",
      "\n",
      "Epoch 00054: saving model to weights.hdf5\n",
      "Epoch 55/100\n",
      "320000/320000 [==============================] - 90s 280us/step - loss: 0.1490 - accuracy: 0.9377 - val_loss: 0.1487 - val_accuracy: 0.9381\n",
      "\n",
      "Epoch 00055: saving model to weights.hdf5\n",
      "Epoch 56/100\n",
      "320000/320000 [==============================] - 89s 279us/step - loss: 0.1487 - accuracy: 0.9377 - val_loss: 0.1480 - val_accuracy: 0.9384\n",
      "\n",
      "Epoch 00056: saving model to weights.hdf5\n",
      "Epoch 57/100\n",
      "320000/320000 [==============================] - 89s 279us/step - loss: 0.1485 - accuracy: 0.9376 - val_loss: 0.1484 - val_accuracy: 0.9386\n",
      "\n",
      "Epoch 00057: saving model to weights.hdf5\n",
      "Epoch 58/100\n",
      "320000/320000 [==============================] - 87s 272us/step - loss: 0.1484 - accuracy: 0.9377 - val_loss: 0.1483 - val_accuracy: 0.9382\n",
      "\n",
      "Epoch 00058: saving model to weights.hdf5\n",
      "Epoch 59/100\n",
      "320000/320000 [==============================] - 88s 275us/step - loss: 0.1473 - accuracy: 0.9380 - val_loss: 0.1478 - val_accuracy: 0.9386\n",
      "\n",
      "Epoch 00059: saving model to weights.hdf5\n",
      "Epoch 60/100\n",
      "320000/320000 [==============================] - 88s 275us/step - loss: 0.1470 - accuracy: 0.9382 - val_loss: 0.1473 - val_accuracy: 0.9389\n",
      "\n",
      "Epoch 00060: saving model to weights.hdf5\n",
      "Epoch 61/100\n",
      "320000/320000 [==============================] - 90s 280us/step - loss: 0.1470 - accuracy: 0.9382 - val_loss: 0.1475 - val_accuracy: 0.9385\n",
      "\n",
      "Epoch 00061: saving model to weights.hdf5\n",
      "Epoch 62/100\n",
      "320000/320000 [==============================] - 91s 283us/step - loss: 0.1467 - accuracy: 0.9383 - val_loss: 0.1473 - val_accuracy: 0.9386\n",
      "\n",
      "Epoch 00062: saving model to weights.hdf5\n",
      "Epoch 63/100\n",
      "320000/320000 [==============================] - 89s 279us/step - loss: 0.1463 - accuracy: 0.9385 - val_loss: 0.1470 - val_accuracy: 0.9386\n",
      "\n",
      "Epoch 00063: saving model to weights.hdf5\n",
      "Epoch 64/100\n",
      "320000/320000 [==============================] - 88s 276us/step - loss: 0.1463 - accuracy: 0.9384 - val_loss: 0.1475 - val_accuracy: 0.9387\n",
      "\n",
      "Epoch 00064: saving model to weights.hdf5\n",
      "Epoch 65/100\n",
      "320000/320000 [==============================] - 89s 279us/step - loss: 0.1461 - accuracy: 0.9387 - val_loss: 0.1474 - val_accuracy: 0.9388\n",
      "\n",
      "Epoch 00065: saving model to weights.hdf5\n",
      "Epoch 66/100\n",
      "320000/320000 [==============================] - 94s 294us/step - loss: 0.1455 - accuracy: 0.9385 - val_loss: 0.1471 - val_accuracy: 0.9391\n",
      "\n",
      "Epoch 00066: saving model to weights.hdf5\n",
      "Epoch 67/100\n",
      "320000/320000 [==============================] - 87s 271us/step - loss: 0.1455 - accuracy: 0.9387 - val_loss: 0.1471 - val_accuracy: 0.9389\n",
      "\n",
      "Epoch 00067: saving model to weights.hdf5\n",
      "Epoch 68/100\n",
      "320000/320000 [==============================] - 93s 290us/step - loss: 0.1451 - accuracy: 0.9387 - val_loss: 0.1469 - val_accuracy: 0.9391\n",
      "\n",
      "Epoch 00068: saving model to weights.hdf5\n",
      "Epoch 69/100\n",
      "320000/320000 [==============================] - 90s 282us/step - loss: 0.1442 - accuracy: 0.9391 - val_loss: 0.1467 - val_accuracy: 0.9393\n",
      "\n",
      "Epoch 00069: saving model to weights.hdf5\n",
      "Epoch 70/100\n",
      "320000/320000 [==============================] - 90s 282us/step - loss: 0.1442 - accuracy: 0.9391 - val_loss: 0.1471 - val_accuracy: 0.9391\n",
      "\n",
      "Epoch 00070: saving model to weights.hdf5\n",
      "Epoch 71/100\n",
      "320000/320000 [==============================] - 90s 282us/step - loss: 0.1445 - accuracy: 0.9392 - val_loss: 0.1464 - val_accuracy: 0.9390\n",
      "\n",
      "Epoch 00071: saving model to weights.hdf5\n",
      "Epoch 72/100\n",
      "320000/320000 [==============================] - 95s 298us/step - loss: 0.1439 - accuracy: 0.9392 - val_loss: 0.1465 - val_accuracy: 0.9391\n",
      "\n",
      "Epoch 00072: saving model to weights.hdf5\n",
      "Epoch 73/100\n",
      "320000/320000 [==============================] - 93s 290us/step - loss: 0.1439 - accuracy: 0.9392 - val_loss: 0.1467 - val_accuracy: 0.9389\n",
      "\n",
      "Epoch 00073: saving model to weights.hdf5\n",
      "Epoch 74/100\n",
      "320000/320000 [==============================] - 91s 284us/step - loss: 0.1432 - accuracy: 0.9396 - val_loss: 0.1463 - val_accuracy: 0.9392\n",
      "\n",
      "Epoch 00074: saving model to weights.hdf5\n",
      "Epoch 75/100\n",
      "320000/320000 [==============================] - 92s 286us/step - loss: 0.1429 - accuracy: 0.9397 - val_loss: 0.1461 - val_accuracy: 0.9393\n",
      "\n",
      "Epoch 00075: saving model to weights.hdf5\n",
      "Epoch 76/100\n",
      "320000/320000 [==============================] - 91s 285us/step - loss: 0.1428 - accuracy: 0.9397 - val_loss: 0.1468 - val_accuracy: 0.9394\n",
      "\n",
      "Epoch 00076: saving model to weights.hdf5\n",
      "Epoch 77/100\n",
      "320000/320000 [==============================] - 90s 282us/step - loss: 0.1421 - accuracy: 0.9399 - val_loss: 0.1464 - val_accuracy: 0.9394\n",
      "\n",
      "Epoch 00077: saving model to weights.hdf5\n",
      "Epoch 78/100\n",
      "320000/320000 [==============================] - 87s 272us/step - loss: 0.1423 - accuracy: 0.9398 - val_loss: 0.1459 - val_accuracy: 0.9396\n",
      "\n",
      "Epoch 00078: saving model to weights.hdf5\n",
      "Epoch 79/100\n",
      "320000/320000 [==============================] - 91s 284us/step - loss: 0.1420 - accuracy: 0.9401 - val_loss: 0.1458 - val_accuracy: 0.9395\n",
      "\n",
      "Epoch 00079: saving model to weights.hdf5\n",
      "Epoch 80/100\n",
      "320000/320000 [==============================] - 87s 272us/step - loss: 0.1418 - accuracy: 0.9400 - val_loss: 0.1458 - val_accuracy: 0.9393\n",
      "\n",
      "Epoch 00080: saving model to weights.hdf5\n",
      "Epoch 81/100\n",
      "320000/320000 [==============================] - 86s 268us/step - loss: 0.1412 - accuracy: 0.9403 - val_loss: 0.1463 - val_accuracy: 0.9393\n",
      "\n",
      "Epoch 00081: saving model to weights.hdf5\n",
      "Epoch 82/100\n",
      "320000/320000 [==============================] - 85s 267us/step - loss: 0.1411 - accuracy: 0.9402 - val_loss: 0.1461 - val_accuracy: 0.9385\n",
      "\n",
      "Epoch 00082: saving model to weights.hdf5\n",
      "Epoch 83/100\n",
      "320000/320000 [==============================] - 86s 267us/step - loss: 0.1409 - accuracy: 0.9405 - val_loss: 0.1454 - val_accuracy: 0.9399\n",
      "\n",
      "Epoch 00083: saving model to weights.hdf5\n",
      "Epoch 84/100\n",
      "320000/320000 [==============================] - 84s 263us/step - loss: 0.1409 - accuracy: 0.9405 - val_loss: 0.1458 - val_accuracy: 0.9395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00084: saving model to weights.hdf5\n",
      "Epoch 85/100\n",
      "320000/320000 [==============================] - 85s 264us/step - loss: 0.1407 - accuracy: 0.9405 - val_loss: 0.1452 - val_accuracy: 0.9398\n",
      "\n",
      "Epoch 00085: saving model to weights.hdf5\n",
      "Epoch 86/100\n",
      "320000/320000 [==============================] - 85s 265us/step - loss: 0.1407 - accuracy: 0.9404 - val_loss: 0.1453 - val_accuracy: 0.9397\n",
      "\n",
      "Epoch 00086: saving model to weights.hdf5\n",
      "Epoch 87/100\n",
      "320000/320000 [==============================] - 85s 266us/step - loss: 0.1401 - accuracy: 0.9407 - val_loss: 0.1455 - val_accuracy: 0.9396\n",
      "\n",
      "Epoch 00087: saving model to weights.hdf5\n",
      "Epoch 88/100\n",
      "320000/320000 [==============================] - 85s 264us/step - loss: 0.1399 - accuracy: 0.9408 - val_loss: 0.1451 - val_accuracy: 0.9396\n",
      "\n",
      "Epoch 00088: saving model to weights.hdf5\n",
      "Epoch 89/100\n",
      "320000/320000 [==============================] - 84s 264us/step - loss: 0.1399 - accuracy: 0.9406 - val_loss: 0.1448 - val_accuracy: 0.9399\n",
      "\n",
      "Epoch 00089: saving model to weights.hdf5\n",
      "Epoch 90/100\n",
      "320000/320000 [==============================] - 86s 269us/step - loss: 0.1396 - accuracy: 0.9408 - val_loss: 0.1450 - val_accuracy: 0.9399\n",
      "\n",
      "Epoch 00090: saving model to weights.hdf5\n",
      "Epoch 91/100\n",
      "320000/320000 [==============================] - 88s 275us/step - loss: 0.1393 - accuracy: 0.9411 - val_loss: 0.1447 - val_accuracy: 0.9398\n",
      "\n",
      "Epoch 00091: saving model to weights.hdf5\n",
      "Epoch 92/100\n",
      "320000/320000 [==============================] - 91s 284us/step - loss: 0.1389 - accuracy: 0.9411 - val_loss: 0.1448 - val_accuracy: 0.9401\n",
      "\n",
      "Epoch 00092: saving model to weights.hdf5\n",
      "Epoch 93/100\n",
      "320000/320000 [==============================] - 89s 277us/step - loss: 0.1388 - accuracy: 0.9410 - val_loss: 0.1453 - val_accuracy: 0.9398\n",
      "\n",
      "Epoch 00093: saving model to weights.hdf5\n",
      "Epoch 94/100\n",
      "320000/320000 [==============================] - 89s 279us/step - loss: 0.1388 - accuracy: 0.9410 - val_loss: 0.1448 - val_accuracy: 0.9401\n",
      "\n",
      "Epoch 00094: saving model to weights.hdf5\n",
      "Epoch 95/100\n",
      "320000/320000 [==============================] - 92s 286us/step - loss: 0.1386 - accuracy: 0.9413 - val_loss: 0.1447 - val_accuracy: 0.9399\n",
      "\n",
      "Epoch 00095: saving model to weights.hdf5\n",
      "Epoch 96/100\n",
      "320000/320000 [==============================] - 92s 286us/step - loss: 0.1381 - accuracy: 0.9414 - val_loss: 0.1446 - val_accuracy: 0.9399\n",
      "\n",
      "Epoch 00096: saving model to weights.hdf5\n",
      "Epoch 97/100\n",
      "320000/320000 [==============================] - 90s 283us/step - loss: 0.1380 - accuracy: 0.9414 - val_loss: 0.1454 - val_accuracy: 0.9400\n",
      "\n",
      "Epoch 00097: saving model to weights.hdf5\n",
      "Epoch 98/100\n",
      "320000/320000 [==============================] - 91s 286us/step - loss: 0.1378 - accuracy: 0.9417 - val_loss: 0.1447 - val_accuracy: 0.9401\n",
      "\n",
      "Epoch 00098: saving model to weights.hdf5\n",
      "Epoch 99/100\n",
      "320000/320000 [==============================] - 90s 282us/step - loss: 0.1375 - accuracy: 0.9414 - val_loss: 0.1446 - val_accuracy: 0.9400\n",
      "\n",
      "Epoch 00099: saving model to weights.hdf5\n",
      "Epoch 100/100\n",
      "320000/320000 [==============================] - 92s 286us/step - loss: 0.1374 - accuracy: 0.9416 - val_loss: 0.1448 - val_accuracy: 0.9402\n",
      "\n",
      "Epoch 00100: saving model to weights.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x170c99e6f98>"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# settings for loggin, nice to use with tensorboard\n",
    "filepath = \"weights.hdf5\"\n",
    "#checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, mode='max')\n",
    "tboard = TensorBoard(log_dir='logs', write_graph=True, write_images=True)\n",
    "callbacks_list = [checkpoint, tboard]\n",
    "\n",
    "#train the brain, maybe best is 200 epochs and 1000 batch size\n",
    "network.fit(x_train, y_train, epochs=100, batch_size=1000, validation_data=(x_test, y_test), callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "286\n",
      "English: 1.01%\n",
      "French: 1.77%\n",
      "German: 30.05%\n",
      "Romanian: 67.18%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    # make prediction and show guess percents\n",
    "    #ă ü\n",
    "    dic = []\n",
    "    dic.append(unidecode(\"gesundkeit\").lower())\n",
    "    vct_str = convert_dic_to_vector(dic, max_letters-1)\n",
    "    vct = np.zeros((1, (char_count * max_letters)-1))\n",
    "    count = 0\n",
    "    print(len(vct_str[0]))\n",
    "    for digit in vct_str[0]:\n",
    "        vct[0,count] = int(digit)\n",
    "        count += 1\n",
    "    prediction_vct = network.predict(vct)\n",
    "\n",
    "    langs = list(label_names)\n",
    "    for i in range(len(label_names)):\n",
    "        lang = langs[i]\n",
    "        score = prediction_vct[0][i]\n",
    "        print(lang + ': ' + str(round(100*score, 2)) + '%')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explicitly save model\n",
    "network.save('lang_detect_n2.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete to test loading of model\n",
    "del network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "network = load_model('lang_detect_long.hdf5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-pythondata] *",
   "language": "python",
   "name": "conda-env-.conda-pythondata-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
