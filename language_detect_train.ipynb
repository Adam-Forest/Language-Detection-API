{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer, one_hot\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "import keras.optimizers\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn.datasets as skds\n",
    "from pathlib import Path\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import os\n",
    "from unidecode import unidecode\n",
    "\n",
    "max_letters=12\n",
    "char_count=84"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['English', 'French', 'German', 'Romanian']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>wikitext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>English</td>\n",
       "      <td>History of the Jews in Romania The history of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>French</td>\n",
       "      <td>La Roche-sur-Yon La Roche-sur-Yon est une com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>German</td>\n",
       "      <td>Paris Paris ist die Hauptstadt der Französisc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Romanian</td>\n",
       "      <td>Avrig Avrig în dialectul săsesc Frek Fraek în...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   language                                           wikitext\n",
       "0   English   History of the Jews in Romania The history of...\n",
       "1    French   La Roche-sur-Yon La Roche-sur-Yon est une com...\n",
       "2    German   Paris Paris ist die Hauptstadt der Französisc...\n",
       "3  Romanian   Avrig Avrig în dialectul săsesc Frek Fraek în..."
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_names = os.listdir('./training/clean/')\n",
    "# Create Dictionary for language (= File name) and Text -- rename the files such as: en_clean.txt = english etc.\n",
    "file_name_and_text = {}\n",
    "for file in file_names:\n",
    "    with open('./training/clean/' + file, \"r\",encoding=\"utf8\" ) as target_file:\n",
    "         file_name_and_text[file] = target_file.read()\n",
    "data_grouped = (pd.DataFrame.from_dict(file_name_and_text, orient='index')\n",
    "             .reset_index().rename(index = str, columns = {'index': 'language', 0: 'wikitext'}))\n",
    "label_names = data_grouped['language'].tolist()\n",
    "print(label_names)\n",
    "data_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "def process(page_content, max_word_length):\n",
    "    count=0\n",
    "    # unidecode will replace special chars like ă or ü with a or u\n",
    "    #page_content = unidecode(page_content)\n",
    "    words = re.sub(r'[^a-zA-Z ]', ' ', page_content)\n",
    "    lower = words.lower()\n",
    "    word_list = lower.split()\n",
    "    short_words = []\n",
    "    for word in word_list:\n",
    "        # set to lower number for testing, limits total number of words used\n",
    "        if count >= 100000:\n",
    "            break\n",
    "        if len(word) <= max_word_length and len(word) > 2:\n",
    "            short_words.append(word)\n",
    "            count=count+1\n",
    "    \n",
    "    # sorting may be better for machine learning but will cause loss of end of alphabet words if full list is not used\n",
    "    #short_words=list(set(short_words))\n",
    "    #short_words.sort()\n",
    "    return short_words\n",
    "\n",
    "test= process(data_grouped.loc[data_grouped.language == 'English','wikitext'].tolist()[0], 12)\n",
    "print(len(test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode words here\n",
    "def convert_dic_to_vector(dic, max_word_length):\n",
    "    new_list = []\n",
    "    for word in dic:\n",
    "        vec = ''\n",
    "        n = len(word)\n",
    "        for i in range(n):\n",
    "            current_letter = word[i]\n",
    "            ind = ord(current_letter)-97\n",
    "            #ind = ord(current_letter)\n",
    "            placeholder = (str(0)*ind) + str(1) + str(0)*((char_count-1)-ind)\n",
    "            vec = vec + placeholder\n",
    "        if n < max_word_length:\n",
    "            excess = max_word_length-n\n",
    "            vec = vec +str(0)*char_count*excess\n",
    "        new_list.append(vec)\n",
    "    print(len(new_list))\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test special charactor values here\n",
    "ord(\"ă\")-97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode words here\n",
    "def create_output_vector(tag_index, number_of_languages):\n",
    "    out = str(0)*tag_index + str(1) + str(0)*(number_of_languages-1-tag_index)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating dictionary for English\n",
      "100000\n",
      "generating dictionary for French\n",
      "100000\n",
      "generating dictionary for German\n",
      "100000\n",
      "generating dictionary for Romanian\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "# build dictionaries to train from\n",
    "word_data = []\n",
    "language_data = []\n",
    "master_dic = []\n",
    "\n",
    "count = 0\n",
    "\n",
    "for lang in label_names:\n",
    "    print('generating dictionary for ' + lang)\n",
    "    dic = process(data_grouped.loc[data_grouped.language == lang,'wikitext'].tolist()[0], max_letters)\n",
    "    for word in dic:\n",
    "        master_dic.append(word)\n",
    "    vct = convert_dic_to_vector(dic, max_letters)\n",
    "    for vector in vct:\n",
    "        word_data.append(vector)\n",
    "    output_vct = create_output_vector(count, len(label_names))\n",
    "    for i in range(len(vct)):\n",
    "        language_data.append(output_vct)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_grouped.loc[data_grouped.language == \"romanian\",'wikitext'].tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoded dataframe\n",
    "arr = []\n",
    "for i in range(len(word_data)):\n",
    "    entry = []\n",
    "    entry.append(master_dic[i])\n",
    "    for digit in language_data[i]:\n",
    "        entry.append(float(digit))\n",
    "    for digit in word_data[i]:\n",
    "        entry.append(float(digit))\n",
    "    arr.append(entry)\n",
    "    #print(entry)\n",
    "\n",
    "\n",
    "arr = np.array(arr)\n",
    "np.save('arr.npy', arr)\n",
    "#df=pd.DataFrame(arr)\n",
    "#df.to_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(arr)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('arr.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into train and test, verify array shapes\n",
    "\n",
    "inputs = data[:, 2+len(label_names):]\n",
    "labels = data[:, 1:1+len(label_names)]\n",
    "\n",
    "#inputs = data[:, 6:]\n",
    "#labels = data[:, 1:5]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(inputs, labels, test_size=0.20)\n",
    "\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the brain\n",
    "network = Sequential()\n",
    "network.add(Dense(200, input_dim=(char_count*max_letters)-1, activation='sigmoid'))\n",
    "network.add(Dense(150, activation='sigmoid'))\n",
    "network.add(Dense(100, activation='sigmoid'))\n",
    "network.add(Dense(100, activation='sigmoid'))\n",
    "network.add(Dense(len(label_names), activation='softmax'))\n",
    "\n",
    "network.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings for loggin, nice to use with tensorboard\n",
    "filepath = \"weights.hdf5\"\n",
    "#checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, mode='max')\n",
    "tboard = TensorBoard(log_dir='logs', write_graph=True, write_images=True)\n",
    "callbacks_list = [checkpoint, tboard]\n",
    "\n",
    "#train the brain, maybe best is 200 epochs and 1000 batch size\n",
    "network.fit(x_train, y_train, epochs=200, batch_size=1000, validation_data=(x_test, y_test), callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1144\n",
      "English: 90.4%\n",
      "French: 5.07%\n",
      "German: 3.19%\n",
      "Romanian: 1.35%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    # make prediction and show guess percents\n",
    "    #ă ü\n",
    "    dic = []\n",
    "    dic.append(\"bonjour\")\n",
    "    vct_str = convert_dic_to_vector(dic, max_letters-1)\n",
    "    vct = np.zeros((1, (char_count * max_letters)-1))\n",
    "    count = 0\n",
    "    print(len(vct_str[0]))\n",
    "    for digit in vct_str[0]:\n",
    "        vct[0,count] = int(digit)\n",
    "        count += 1\n",
    "    prediction_vct = network.predict(vct)\n",
    "\n",
    "    langs = list(label_names)\n",
    "    for i in range(len(label_names)):\n",
    "        lang = langs[i]\n",
    "        score = prediction_vct[0][i]\n",
    "        print(lang + ': ' + str(round(100*score, 2)) + '%')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explicitly save model\n",
    "network.save('lang_detect.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete to test loading of model\n",
    "del network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "network = load_model('lang_detect.hdf5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-pythondata] *",
   "language": "python",
   "name": "conda-env-.conda-pythondata-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
