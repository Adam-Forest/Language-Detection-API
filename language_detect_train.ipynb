{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer, one_hot\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "import keras.optimizers\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn.datasets as skds\n",
    "from pathlib import Path\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "\n",
    "max_letters=12\n",
    "char_count=104"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source file directory\n",
    "path_train = \"./training\"\n",
    " \n",
    "files_train = skds.load_files(path_train,load_content=False)\n",
    " \n",
    "label_index = files_train.target\n",
    "label_names = files_train.target_names\n",
    "labelled_files = files_train.filenames\n",
    "\n",
    "data_tags = [\"language\",\"wikitext\"]\n",
    "data_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['language', 'wikitext']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and add data from file to a list\n",
    "i=0\n",
    "for f in labelled_files:\n",
    "    # data_list.append((f,label_names[label_index[i]],Path(f).read_text(encoding=\"utf8\")))\n",
    "    data_list.append((label_names[label_index[i]],Path(f).read_text(encoding=\"utf8\")))\n",
    "    i += 1\n",
    " \n",
    "# We have training data available as dictionary filename, category, data\n",
    "data = pd.DataFrame.from_records(data_list, columns=data_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>wikitext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>english</td>\n",
       "      <td>Zealand and Australia for the first time The b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>french</td>\n",
       "      <td>parlement de Bosnie-Herzégovine Les électeurs ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>german</td>\n",
       "      <td>die westliche Linke könne die Sowjetunion kein...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>romanian</td>\n",
       "      <td>în secolul al XIV-lea și comiți ai comitatului...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   language                                           wikitext\n",
       "0   english  Zealand and Australia for the first time The b...\n",
       "1    french  parlement de Bosnie-Herzégovine Les électeurs ...\n",
       "2    german  die westliche Linke könne die Sowjetunion kein...\n",
       "3  romanian  în secolul al XIV-lea și comiți ai comitatului..."
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instead of all this, should just stick with unsplit data files\n",
    "\n",
    "data_english = data.loc[data['language'] == \"english\"].reset_index(drop=True)\n",
    "data_german = data.loc[data['language'] == \"german\"].reset_index(drop=True)\n",
    "data_french = data.loc[data['language'] == \"french\"].reset_index(drop=True)\n",
    "data_romanian = data.loc[data['language'] == \"romanian\"].reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "data_sorted=pd.concat([data_english, data_german, data_french, data_romanian]).sort_index(kind='mergesort').reset_index(drop=True)\n",
    "data_sorted.head(20)\n",
    "data_english.head()\n",
    "data_english_m=data_english.groupby('language')['wikitext'].apply(' '.join).reset_index()\n",
    "data_french_m=data_french.groupby('language')['wikitext'].apply(' '.join).reset_index()\n",
    "data_german_m=data_german.groupby('language')['wikitext'].apply(' '.join).reset_index()\n",
    "data_romanian_m=data_romanian.groupby('language')['wikitext'].apply(' '.join).reset_index()\n",
    "\n",
    "data_grouped=data.groupby('language')['wikitext'].apply(' '.join).reset_index()\n",
    "data_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "def process(page_content, max_word_length):\n",
    "    count=0\n",
    "    # unidecode will replace special chars like ă or ü with a or u\n",
    "    #page_content = unidecode(page_content)\n",
    "    words = re.sub(r'[^a-zA-Z ]', ' ', page_content)\n",
    "    lower = words.lower()\n",
    "    word_list = lower.split()\n",
    "    short_words = []\n",
    "    for word in word_list:\n",
    "        # set to lower number for testing, limits total number of words used\n",
    "        if count >= 10000:\n",
    "            break\n",
    "        if len(word) <= max_word_length and len(word) > 2:\n",
    "            short_words.append(word)\n",
    "            count=count+1\n",
    "    \n",
    "    # sorting may be better for machine learning but will cause loss of end of alphabet words if full list is not used\n",
    "    #short_words=list(set(short_words))\n",
    "    #short_words.sort()\n",
    "    return short_words\n",
    "\n",
    "test= process(data_grouped.loc[data_grouped.language == 'english','wikitext'].tolist()[0], 12)\n",
    "print(len(test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode words here\n",
    "def convert_dic_to_vector(dic, max_word_length):\n",
    "    new_list = []\n",
    "    for word in dic:\n",
    "        vec = ''\n",
    "        n = len(word)\n",
    "        for i in range(n):\n",
    "            current_letter = word[i]\n",
    "            ind = ord(current_letter)-97\n",
    "            #ind = ord(current_letter)\n",
    "            placeholder = (str(0)*ind) + str(1) + str(0)*((char_count-1)-ind)\n",
    "            vec = vec + placeholder\n",
    "        if n < max_word_length:\n",
    "            excess = max_word_length-n\n",
    "            vec = vec +str(0)*char_count*excess\n",
    "        new_list.append(vec)\n",
    "    print(len(new_list))\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test special charactor values here\n",
    "ord(\"ă\")-97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode words here\n",
    "def create_output_vector(tag_index, number_of_languages):\n",
    "    out = str(0)*tag_index + str(1) + str(0)*(number_of_languages-1-tag_index)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating dictionary for english\n",
      "10000\n",
      "generating dictionary for french\n",
      "10000\n",
      "generating dictionary for german\n",
      "10000\n",
      "generating dictionary for romanian\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "# build dictionaries to train from\n",
    "word_data = []\n",
    "language_data = []\n",
    "master_dic = []\n",
    "\n",
    "count = 0\n",
    "\n",
    "for lang in label_names:\n",
    "    print('generating dictionary for ' + lang)\n",
    "    dic = process(data_grouped.loc[data_grouped.language == lang,'wikitext'].tolist()[0], max_letters)\n",
    "    for word in dic:\n",
    "        master_dic.append(word)\n",
    "    vct = convert_dic_to_vector(dic, max_letters)\n",
    "    for vector in vct:\n",
    "        word_data.append(vector)\n",
    "    output_vct = create_output_vector(count, len(label_names))\n",
    "    for i in range(len(vct)):\n",
    "        language_data.append(output_vct)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_grouped.loc[data_grouped.language == \"romanian\",'wikitext'].tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoded dataframe\n",
    "arr = []\n",
    "for i in range(len(word_data)):\n",
    "    entry = []\n",
    "    entry.append(master_dic[i])\n",
    "    for digit in language_data[i]:\n",
    "        entry.append(float(digit))\n",
    "    for digit in word_data[i]:\n",
    "        entry.append(float(digit))\n",
    "    arr.append(entry)\n",
    "    #print(entry)\n",
    "\n",
    "\n",
    "arr = np.array(arr)\n",
    "np.save('arr.npy', arr)\n",
    "#df=pd.DataFrame(arr)\n",
    "#df.to_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1243</th>\n",
       "      <th>1244</th>\n",
       "      <th>1245</th>\n",
       "      <th>1246</th>\n",
       "      <th>1247</th>\n",
       "      <th>1248</th>\n",
       "      <th>1249</th>\n",
       "      <th>1250</th>\n",
       "      <th>1251</th>\n",
       "      <th>1252</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>zealand</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>and</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>australia</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>for</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1253 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0    1    2    3    4    5    6    7    8    9     ... 1243 1244 1245  \\\n",
       "0    zealand  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "1        and  1.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "2  australia  1.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "3        for  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "4        the  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "\n",
       "  1246 1247 1248 1249 1250 1251 1252  \n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 1253 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame(arr)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('arr.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 1247)\n",
      "(8000, 4)\n",
      "(32000, 1247)\n",
      "(32000, 4)\n"
     ]
    }
   ],
   "source": [
    "#split into train and test, verify array shapes\n",
    "\n",
    "inputs = data[:, 2+len(label_names):]\n",
    "labels = data[:, 1:1+len(label_names)]\n",
    "\n",
    "#inputs = data[:, 6:]\n",
    "#labels = data[:, 1:5]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(inputs, labels, test_size=0.20)\n",
    "\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the brain\n",
    "network = Sequential()\n",
    "network.add(Dense(200, input_dim=(char_count*max_letters)-1, activation='sigmoid'))\n",
    "network.add(Dense(150, activation='sigmoid'))\n",
    "network.add(Dense(100, activation='sigmoid'))\n",
    "network.add(Dense(100, activation='sigmoid'))\n",
    "network.add(Dense(len(label_names), activation='softmax'))\n",
    "\n",
    "network.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/10\n",
      "32000/32000 [==============================] - 25s 785us/step - loss: 0.5711 - accuracy: 0.7500 - val_loss: 0.5621 - val_accuracy: 0.7500\n",
      "\n",
      "Epoch 00001: saving model to weights.hdf5\n",
      "Epoch 2/10\n",
      "32000/32000 [==============================] - 25s 794us/step - loss: 0.5623 - accuracy: 0.7500 - val_loss: 0.5618 - val_accuracy: 0.7500\n",
      "\n",
      "Epoch 00002: saving model to weights.hdf5\n",
      "Epoch 3/10\n",
      "32000/32000 [==============================] - 24s 759us/step - loss: 0.5616 - accuracy: 0.7500 - val_loss: 0.5613 - val_accuracy: 0.7500\n",
      "\n",
      "Epoch 00003: saving model to weights.hdf5\n",
      "Epoch 4/10\n",
      "32000/32000 [==============================] - 24s 765us/step - loss: 0.5588 - accuracy: 0.7500 - val_loss: 0.5543 - val_accuracy: 0.7500\n",
      "\n",
      "Epoch 00004: saving model to weights.hdf5\n",
      "Epoch 5/10\n",
      "32000/32000 [==============================] - 25s 794us/step - loss: 0.5379 - accuracy: 0.7534 - val_loss: 0.5131 - val_accuracy: 0.7659\n",
      "\n",
      "Epoch 00005: saving model to weights.hdf5\n",
      "Epoch 6/10\n",
      "32000/32000 [==============================] - 24s 761us/step - loss: 0.4864 - accuracy: 0.7748 - val_loss: 0.4665 - val_accuracy: 0.7872\n",
      "\n",
      "Epoch 00006: saving model to weights.hdf5\n",
      "Epoch 7/10\n",
      "32000/32000 [==============================] - 23s 731us/step - loss: 0.4471 - accuracy: 0.7950 - val_loss: 0.4453 - val_accuracy: 0.7955\n",
      "\n",
      "Epoch 00007: saving model to weights.hdf5\n",
      "Epoch 8/10\n",
      "32000/32000 [==============================] - 23s 733us/step - loss: 0.4246 - accuracy: 0.8076 - val_loss: 0.4242 - val_accuracy: 0.8095\n",
      "\n",
      "Epoch 00008: saving model to weights.hdf5\n",
      "Epoch 9/10\n",
      "32000/32000 [==============================] - 24s 735us/step - loss: 0.4066 - accuracy: 0.8226 - val_loss: 0.4078 - val_accuracy: 0.8238\n",
      "\n",
      "Epoch 00009: saving model to weights.hdf5\n",
      "Epoch 10/10\n",
      "32000/32000 [==============================] - 24s 734us/step - loss: 0.3947 - accuracy: 0.8307 - val_loss: 0.4026 - val_accuracy: 0.8261\n",
      "\n",
      "Epoch 00010: saving model to weights.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1b70d4354e0>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# settings for loggin, nice to use with tensorboard\n",
    "filepath = \"weights.hdf5\"\n",
    "#checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, mode='max')\n",
    "tboard = TensorBoard(log_dir='logs', write_graph=True, write_images=True)\n",
    "callbacks_list = [checkpoint, tboard]\n",
    "\n",
    "#train the brain, maybe best is 200 epochs and 1000 batch size\n",
    "network.fit(x_train, y_train, epochs=10, batch_size=1000, validation_data=(x_test, y_test), callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1144\n",
      "english: 79.38%\n",
      "french: 16.13%\n",
      "german: 1.0%\n",
      "romanian: 3.49%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    # make prediction and show guess percents\n",
    "    #ă ü\n",
    "    dic = []\n",
    "    dic.append(\"bonjour\")\n",
    "    vct_str = convert_dic_to_vector(dic, max_letters-1)\n",
    "    vct = np.zeros((1, (char_count * max_letters)-1))\n",
    "    count = 0\n",
    "    print(len(vct_str[0]))\n",
    "    for digit in vct_str[0]:\n",
    "        vct[0,count] = int(digit)\n",
    "        count += 1\n",
    "    prediction_vct = network.predict(vct)\n",
    "\n",
    "    langs = list(label_names)\n",
    "    for i in range(len(label_names)):\n",
    "        lang = langs[i]\n",
    "        score = prediction_vct[0][i]\n",
    "        print(lang + ': ' + str(round(100*score, 2)) + '%')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explicitly save model\n",
    "network.save('lang_detect.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete to test loading of model\n",
    "del network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "network = load_model('lang_detect.hdf5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-pythondata] *",
   "language": "python",
   "name": "conda-env-.conda-pythondata-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
